{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackabuse.com/python-for-nlp-getting-started-with-the-stanfordcorenlp-library/ <br>\n",
    "https://github.com/Lynten/stanford-corenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('newsdataset1.csv')\n",
    "df.event_summary=df.event_summary.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The death toll of the suicide bombing in Ibb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['event_summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "POS: [('The', 'DT'), ('death', 'NN'), ('toll', 'NN'), ('of', 'IN'), ('the', 'DT'), ('suicide', 'NN'), ('bombing', 'NN'), ('in', 'IN'), ('Ibb', 'NNP')]\n",
      "\n",
      "\n",
      "\n",
      "Tokens: ['The', 'death', 'toll', 'of', 'the', 'suicide', 'bombing', 'in', 'Ibb']\n",
      "\n",
      "\n",
      "\n",
      "NER: [('The', 'O'), ('death', 'O'), ('toll', 'O'), ('of', 'O'), ('the', 'O'), ('suicide', 'CRIMINAL_CHARGE'), ('bombing', 'CRIMINAL_CHARGE'), ('in', 'O'), ('Ibb', 'O')]\n",
      "\n",
      "\n",
      "\n",
      "Parse: (ROOT\n",
      "  (NP\n",
      "    (NP (DT The) (NN death))\n",
      "    (NP\n",
      "      (NP (NN toll))\n",
      "      (PP (IN of)\n",
      "        (NP\n",
      "          (NP (DT the) (NN suicide) (NN bombing))\n",
      "          (PP (IN in)\n",
      "            (NP (NNP Ibb))))))))\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sNLP = StanfordNLP()\n",
    "    text = df['event_summary'][0]\n",
    "    #print (\"Annotate:\", sNLP.annotate(text))\n",
    "    print('\\n\\n')\n",
    "    print (\"POS:\", sNLP.pos(text))\n",
    "    print('\\n\\n')\n",
    "    print (\"Tokens:\", sNLP.word_tokenize(text))\n",
    "    print('\\n\\n')\n",
    "    print (\"NER:\", sNLP.ner(text))\n",
    "    print('\\n\\n')\n",
    "    print (\"Parse:\", sNLP.parse(text))\n",
    "    print('\\n\\n')\n",
    "    #print (\"Dep Parse:\", sNLP.dependency_parse(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos tags\n",
    "CC Coordinating conjunction\n",
    "CD Cardinal number\n",
    "DT Determiner\n",
    "EX Existential there\n",
    "FW Foreign word\n",
    "IN Preposition or subordinating conjunction\n",
    "JJ Adjective\n",
    "JJR Adjective, comparative\n",
    "JJS Adjective, superlative\n",
    "LS List item marker\n",
    "MD Modal\n",
    "NN Noun, singular or mass\n",
    "NNS Noun, plural\n",
    "NNP Proper noun, singular\n",
    "NNPS Proper noun, plural\n",
    "PDT Predeterminer\n",
    "POS Possessive ending\n",
    "PRP Personal pronoun\n",
    "PRP Possessive pronoun\n",
    "RB Adverb\n",
    "RBR Adverb, comparative\n",
    "RBS Adverb, superlative\n",
    "RP Particle\n",
    "SYM Symbol\n",
    "TO to\n",
    "UH Interjection\n",
    "VB Verb, base form\n",
    "VBD Verb, past tense\n",
    "VBG Verb, gerund or present participle\n",
    "VBN Verb, past participle\n",
    "VBP Verb, non 3rd person singular present\n",
    "VBZ Verb, 3rd person singular present\n",
    "WDT Wh determiner\n",
    "WP Wh pronoun\n",
    "WP$ Possessive wh pronoun\n",
    "WRB Wh adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanfordcorenlp.server'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a3e41ce1d035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstanfordcorenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m with CoreNLPClient(annotators=[ 'tokenize','ssplit','pos','parse'],\n\u001b[0;32m      4\u001b[0m                    \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0moutput_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"json\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stanfordcorenlp.server'"
     ]
    }
   ],
   "source": [
    "from stanfordnlp.server import CoreNLPClient\n",
    "from nltk.tree import Tree\n",
    "with CoreNLPClient(annotators=[ 'tokenize','ssplit','pos','parse'],\n",
    "                   timeout=30000,\n",
    "                   output_format=\"json\",\n",
    "                   properties={'tokenize.language' :'fr',\n",
    "                               'pos.model' : 'edu/stanford/nlp/models/pos-tagger/english/english.tagger',\n",
    "                               'parse.model' : 'edu/stanford/nlp/models/lexparser/englishFactored.ser.gz'}) as client :\n",
    "    ann = client.annotate(text)\n",
    "\n",
    "output = ann['sentences'][0]['parse']\n",
    "parsetree = Tree.fromstring(output)\n",
    "parsetree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp_wrapper = StanfordCoreNLP('http://localhost:9000')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "annot_doc = nlp_wrapper.annotate(df['event_summary'][0],  \n",
    "    properties={\n",
    "        'annotators': 'ner, pos',\n",
    "        'outputFormat': 'json',\n",
    "        'timeout': 1000,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The => the\n",
      "death => death\n",
      "toll => toll\n",
      "of => of\n",
      "the => the\n",
      "suicide => suicide\n",
      "bombing => bombing\n",
      "in => in\n",
      "Ibb => Ibb\n"
     ]
    }
   ],
   "source": [
    "for sentence in annot_doc[\"sentences\"]:  \n",
    "    for word in sentence[\"tokens\"]:\n",
    "        print(word[\"word\"] + \" => \" + word[\"lemma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The=>DT\n",
      "death=>NN\n",
      "toll=>NN\n",
      "of=>IN\n",
      "the=>DT\n",
      "suicide=>NN\n",
      "bombing=>NN\n",
      "in=>IN\n",
      "Ibb=>NNP\n"
     ]
    }
   ],
   "source": [
    "for sentence in annot_doc[\"sentences\"]:  \n",
    "    for word in sentence[\"tokens\"]:\n",
    "        print (word[\"word\"] + \"=>\" + word[\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The=>O\n",
      "death=>O\n",
      "toll=>O\n",
      "of=>O\n",
      "the=>O\n",
      "suicide=>CRIMINAL_CHARGE\n",
      "bombing=>CRIMINAL_CHARGE\n",
      "in=>O\n",
      "Ibb=>O\n"
     ]
    }
   ],
   "source": [
    "for sentence in annot_doc[\"sentences\"]:  \n",
    "    for word in sentence[\"tokens\"]:\n",
    "        print (word[\"word\"] + \"=>\" + word[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
